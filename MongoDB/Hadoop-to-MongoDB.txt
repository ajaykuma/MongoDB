

If your mongod requires authorization, the user account used by the Hadoop connector must be able to run the splitVector command on the input database. You can either give the user the clusterManager role, or create a custom role for this:
db.createRole({
  role: "hadoopSplitVector",
  privileges: [{
    resource: {
      db: "myDatabase",
      collection: "myCollection"
    },
    actions: ["splitVector"]
  }],
  roles:[]
});
db.createUser({
  user: "hadoopUser",
  pwd: "secret",
  roles: ["hadoopSplitVector", "readWrite"]
});

Note that the splitVector command cannot be run through a mongos, so the Hadoop connector will automatically connect to the primary shard in order to run the command. If your input collection is unsharded and the connector reads through a mongos, make sure that your MongoDB Hadoop user is created on the primary shard as well.

https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-1-introduction-setup?jmp=docs
https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-2-hive-example?jmp=docs
https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-3-spark-example-key-takeaways?jmp=docs


Maven for Hadoop connectors
http://repo1.maven.org/maven2/org/mongodb/mongo-hadoop/

MongoDB Driver
http://mongodb.github.io/mongo-java-driver/?jmp=docs

Move these JARs onto each node of the Hadoop cluster. You may provision each node so that the jars are somewhere on Hadoopâ€™s CLASSPATH (e.g. $HADOOP_PREFIX/share/hadoop/common), or you can use the Hadoop DistributedCache to move the JARs onto pre-existing nodes.


