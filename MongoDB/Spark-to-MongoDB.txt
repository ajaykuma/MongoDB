MongoDB provides us a plugin called the mongo-spark-connector, which will help us connect MongoDB and Spark without any drama at all. We just need to provide the MongoDB connection URI in the SparkConf object, and create a ReadConfig object specifying the collection name. 

================================================================================
OPTION 1:
SETUP & READ DATA FROM MONGODB (FROM SPARK-SHELL)

#download mongodb-spark connector or start spark-shell refering package as shown
#Here my mongodb is running on 192.168.251.11, has a DB "test" and collection "firstdoc" with 2 documents

spark-shell --conf "spark.mongodb.input.uri=mongodb://192.168.251.11/test.firstdoc?readPreference=primaryPreferred" --conf "spark.mongodb.output.uri=mongodb://192.168.251.11/test.firstdoc" --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.1
Ivy Default Cache set to: /home/hdu/.ivy2/cache
The jars for the packages stored in: /home/hdu/.ivy2/jars
:: loading settings :: url = jar:file:/usr/local/spark-2.3.3-bin-hadoop2.6/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.mongodb.spark#mongo-spark-connector_2.11 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-76bca3f9-7160-4361-8f51-b046a8b9eda1;1.0
	confs: [default]
	found org.mongodb.spark#mongo-spark-connector_2.11;2.4.1 in central
	found org.mongodb#mongo-java-driver;3.10.2 in central
	[3.10.2] org.mongodb#mongo-java-driver;[3.10,3.11)
:: resolution report :: resolve 1433ms :: artifacts dl 5ms
	:: modules in use:
	org.mongodb#mongo-java-driver;3.10.2 from central in [default]
	org.mongodb.spark#mongo-spark-connector_2.11;2.4.1 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   2   |   1   |   0   |   0   ||   2   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-76bca3f9-7160-4361-8f51-b046a8b9eda1
	confs: [default]
	0 artifacts copied, 2 already retrieved (0kB/33ms)
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://um1:4040
Spark context available as 'sc' (master = spark://um1:7077, app id = app-20191112040946-0002).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.3.3
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_221)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import com.mongodb.spark._
import com.mongodb.spark._

scala> val rdd = MongoSpark.load(sc)
2019-11-12 04:10:21 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some configuration may not take effect.
rdd: com.mongodb.spark.rdd.MongoRDD[org.bson.Document] = MongoRDD[0] at RDD at MongoRDD.scala:51

scala> println(rdd.count)
2                                                                               

scala> println(rdd.first.toJson)
{"_id": {"$oid": "5dca1f6d69fe342d1702c58e"}, "name": "Europe"}

scala> rdd.take(10).foreach(println)
Document{{_id=5dca1f6d69fe342d1702c58e, name=Europe}}
Document{{_id=5dca1f7569fe342d1702c58f, name=America}}
------------------------------------

To specify a different collection, database, and other read configuration settings, pass a ReadConfig to MongoSpark.load().
Using a ReadConfig

MongoSpark.load() can accept a ReadConfig object which specifies various read configuration settings, such as the collection or the Read Preference.

The following example reads from the spark collection with a secondaryPreferred read preference

import com.mongodb.spark.config._
val readConfig = ReadConfig(Map("collection" -> "spark", "readPreference.name" -> "secondaryPreferred"), Some(ReadConfig(sc)))
val customRdd = MongoSpark.load(sc, readConfig)
println(customRdd.count)
println(customRdd.first.toJson)

For example:
scala> import com.mongodb.spark.config._
import com.mongodb.spark.config._

scala> val readConfig = ReadConfig(Map("collection" -> "seconddoc", "readPreference.name" -> "secondaryPreferred"), Some(ReadConfig(sc)))
readConfig: com.mongodb.spark.config.ReadConfig = ReadConfig(test,seconddoc,Some(mongodb://192.168.251.11/test.firstdoc?readPreference=primaryPreferred),1000,DefaultMongoPartitioner,Map(),15,ReadPreferenceConfig(secondaryPreferred,None),ReadConcernConfig(None),AggregationConfig(None,None,None,true),false,true,250,true,true,10000,None)

scala> val customRdd = MongoSpark.load(sc, readConfig)
2019-11-12 04:22:23 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some configuration may not take effect.
customRdd: com.mongodb.spark.rdd.MongoRDD[org.bson.Document] = MongoRDD[1] at RDD at MongoRDD.scala:51

scala> println(customRdd.count)
3

scala> println(customRdd.first.toJson)
{"_id": {"$oid": "5dca24e37cc24a9e9a89fb3c"}, "name": "America"}

scala> customRdd.take(10).foreach(println)
Document{{_id=5dca24e37cc24a9e9a89fb3c, name=America}}
Document{{_id=5dca24e77cc24a9e9a89fb3d, name=America1}}
Document{{_id=5dca24e97cc24a9e9a89fb3e, name=America2}}
----------------------------------


SparkContext Load Helper Methods

SparkContext has an implicit helper method loadFromMongoDB() to load data from MongoDB.

For example, use the loadFromMongoDB() method without any arguments to load the collection specified in the SparkConf:
sc.loadFromMongoDB() // Uses the SparkConf for configuration
Call loadFromMongoDB() with a ReadConfig object to specify a different MongoDB server address, database and collection.

sc.loadFromMongoDB(ReadConfig(Map("uri" -> "mongodb://example.com/database.collection"))) // Uses the ReadConfig

====================================
WRITE DATA TO MONGODB
When saving RDD data into MongoDB, the data must be convertible to a BSON document
You may need to include a map transformation to convert the data into a Document (or BsonDocument or a DBObject).

The following example creates a 10 document RDD and saves it to the MongoDB collection specified in the SparkConf:
scala> import org.bson.Document
import org.bson.Document

scala> val documents = sc.parallelize((1 to 10).map(i => Document.parse(s"{test: $i}")))
documents: org.apache.spark.rdd.RDD[org.bson.Document] = ParallelCollectionRDD[2] at parallelize at <console>:31

scala> MongoSpark.save(documents) 

scala> val readConfig = ReadConfig(Map("collection" -> "firstdoc", "readPreference.name" -> "secondaryPreferred"), Some(ReadConfig(sc)))
readConfig: com.mongodb.spark.config.ReadConfig = ReadConfig(test,firstdoc,Some(mongodb://192.168.251.11/test.firstdoc?readPreference=primaryPreferred),1000,DefaultMongoPartitioner,Map(),15,ReadPreferenceConfig(secondaryPreferred,None),ReadConcernConfig(None),AggregationConfig(None,None,None,true),false,true,250,true,true,10000,None)

scala> val customRdd = MongoSpark.load(sc, readConfig)
2019-11-12 04:33:10 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some configuration may not take effect.
customRdd: com.mongodb.spark.rdd.MongoRDD[org.bson.Document] = MongoRDD[3] at RDD at MongoRDD.scala:51

scala> 

scala> customRdd.take(10).foreach(println)
Document{{_id=5dca1f6d69fe342d1702c58e, name=Europe}}
Document{{_id=5dca1f7569fe342d1702c58f, name=America}}
Document{{_id=5dca2759e0aabf6f3b225ad9, test=6}}
Document{{_id=5dca2759e0aabf6f3b225ada, test=7}}
Document{{_id=5dca2759353cfb6e13eeb20f, test=3}}
Document{{_id=5dca2759353cfb6e13eeb210, test=4}}
Document{{_id=5dca2759353cfb6e13eeb211, test=5}}
Document{{_id=5dca2759e0aabf6f3b225ad7, test=1}}
Document{{_id=5dca2759e0aabf6f3b225ad8, test=2}}
Document{{_id=5dca2759353cfb6e13eeb212, test=8}}

---------------------------------
Using a WriteConfig

MongoSpark.save() can accept a WriteConfig object which specifies various write configuration settings, such as the collection or the write concern.

For example, the following code saves data to the spark collection with a majority write concern

scala> import com.mongodb.spark.config._
import com.mongodb.spark.config._

scala> val writeConfig = WriteConfig(Map("collection" -> "spark", "writeConcern.w" -> "majority"), Some(WriteConfig(sc)))
writeConfig: com.mongodb.spark.config.WriteConfig = WriteConfig(test,spark,Some(mongodb://192.168.251.11/test.firstdoc),true,512,15,WriteConcernConfig(None,Some(majority),None,None),None,false,true,true)

scala> val sparkDocuments = sc.parallelize((1 to 10).map(i => Document.parse(s"{spark: $i}")))
sparkDocuments: org.apache.spark.rdd.RDD[org.bson.Document] = ParallelCollectionRDD[4] at parallelize at <console>:34

scala> MongoSpark.save(sparkDocuments, writeConfig)

scala> val readConfig = ReadConfig(Map("collection" -> "spark", "readPreference.name" -> "secondaryPreferred"), Some(ReadConfig(sc)))
readConfig: com.mongodb.spark.config.ReadConfig = ReadConfig(test,spark,Some(mongodb://192.168.251.11/test.firstdoc?readPreference=primaryPreferred),1000,DefaultMongoPartitioner,Map(),15,ReadPreferenceConfig(secondaryPreferred,None),ReadConcernConfig(None),AggregationConfig(None,None,None,true),false,true,250,true,true,10000,None)

scala> val customRdd = MongoSpark.load(sc, readConfig)
2019-11-12 04:36:32 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some configuration may not take effect.
customRdd: com.mongodb.spark.rdd.MongoRDD[org.bson.Document] = MongoRDD[5] at RDD at MongoRDD.scala:51

scala> customRdd.take(10).foreach(println)
Document{{_id=5dca28aa353cfb6e13eeb217, spark=6}}
Document{{_id=5dca28aa353cfb6e13eeb218, spark=7}}
Document{{_id=5dca28aa353cfb6e13eeb219, spark=1}}
Document{{_id=5dca28aa353cfb6e13eeb21a, spark=2}}
Document{{_id=5dca28aae0aabf6f3b225ade, spark=3}}
Document{{_id=5dca28aae0aabf6f3b225adf, spark=4}}
Document{{_id=5dca28aae0aabf6f3b225ae0, spark=5}}
Document{{_id=5dca28aae0aabf6f3b225ae1, spark=8}}
Document{{_id=5dca28aae0aabf6f3b225ae2, spark=9}}
Document{{_id=5dca28aae0aabf6f3b225ae3, spark=10}}

--------------------------------------------

RDD Save Helper Methods

RDDs have an implicit helper method saveToMongoDB() to write data to MongoDB:

For example, the following uses the documents RDD defined above and uses its saveToMongoDB() method without any arguments to save the documents to the collection specified in the SparkConf:

documents.saveToMongoDB() // Uses the SparkConf for configuration

Call saveToMongoDB() with a WriteConfig object to specify a different MongoDB server address, database and collection.

documents.saveToMongoDB(WriteConfig(Map("uri" -> "mongodb://example.com/database.collection")))
// Uses the WriteConfig
-------------------------------------------------

Unsupported Types

Some Scala types (e.g. Lists) are unsupported and should be converted to their Java equivalent. To convert from Scala into native types include the following import statement to use the .asJava method:

The following operation imports the .asJava method, converts a Scala list to its Java equivalent, and saves it to MongoDB:

import scala.collection.JavaConverters._
import org.bson.Document

val documents = sc.parallelize(
  Seq(new Document("fruits", List("apples", "oranges", "pears").asJava))
)
MongoSpark.save(documents)

=========================================================

Filters & Aggregation:
https://docs.mongodb.com/spark-connector/master/scala/aggregation/

Datasets & DataFrames:
https://docs.mongodb.com/spark-connector/master/scala/datasets-and-sql/

Spark Streaming:
https://docs.mongodb.com/spark-connector/master/scala/streaming/

==============================================
OPTION 2:
DOWNLOADING THE JARS FROM MAVEN INTO SPARK_HOME/jars

https://mvnrepository.com/artifact/org.mongodb.spark/mongo-spark-connector

hdu@um1:/usr/local/spark$ ls /home/hdu/Downloads/mongo*
/home/hdu/Downloads/mongo-java-driver-3.11.2.jar
/home/hdu/Downloads/mongo-spark-connector_2.11-2.3.3.jar

hdu@um1:/usr/local/spark$ spark-shell --conf "spark.mongodb.input.uri=mongodb://192.168.251.11/test.firstdoc?readPreference=primaryPreferred" --conf "spark.mongodb.output.uri=mongodb://192.168.251.11/test.firstdoc"

==========================================
OPTION 3:
Using IDE and a Build file

For example:
scalaVersion := "2.11.7",
libraryDependencies ++= Seq(
  "org.mongodb.spark" %% "mongo-spark-connector" % "2.4.1",
  "org.apache.spark" %% "spark-core" % "2.4.1",
  "org.apache.spark" %% "spark-sql" % "2.4.1"
)

Example app:
package com.mongodb

object GettingStarted {

  def main(args: Array[String]): Unit = {

    /* Create the SparkSession.
     * If config arguments are passed from the command line using --conf,
     * parse args for the values to set.
     */
    import org.apache.spark.sql.SparkSession

    val spark = SparkSession.builder()
      .master("local")
      .appName("MongoSparkConnectorIntro")
      .config("spark.mongodb.input.uri", "mongodb://127.0.0.1/test.myCollection")
      .config("spark.mongodb.output.uri", "mongodb://127.0.0.1/test.myCollection")
      .getOrCreate()

  }
}

=====================================================================================
